---
title: "V-Matrix nach Vapniks LUSI Learning"
author: "Dr. Robert Opitz"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Theory

Selecting the right loss function can be annoying. Vapnik addressed this problem in [@Vapnik2020] and suggests using only the quadratic loss function. He points out that this is not always the best idea and extends the risk function.

The newly proposed risk function, rearranged by me for didactic reasons, is:

$$
\begin{aligned}
R & = \frac{1}{n}\,\mathbf{r}^{T}\,\mathbf{V\,r} \\
\mathbf{r} & = \mathbf{y} - \mathbf{\hat{y}} \\
\hat{\mathbf{y}} & = h(\mathbf{X})
\end{aligned}
$$

Here $\mathbf{y}$ is a single column matrix of the response with $n$ rows, where $n$ is the number of observations (data points/instances). $\mathbf{X}$ is the matrix
of the (multivariate) explanatory variables with $n$ rows and $m$ columns. The hypothesis function $h$ returns
the predicted response $\hat{\mathbf{y}}$, also a single column matrix with $n$ columns. The difference of $\mathbf{y}$ and $\hat{\mathbf{y}}$ is called the residuals, is a single column matrix of length $n$.

The matrix $\mathbf{V}$ is a quadratic $n \times n$ matrix. The elements of the $\mathbf{V}$ can be defined as:

$$
v_{ij}(x_i, x_j) = \sum_{s=1}^n G(x_s - x_i)\,G(x_s - x_j)
$$

The function $G$ is a square-integrable function. 

The matrix $\mathbf{V}$ can become the unity matrix $\mathbf{I}$.Then the risk becomes the usual mean squared error (and an estimate of the residual variance):

$$
R = \frac{1}{n}\,\mathbf{r}^{T}\,\mathbf{I}\,\mathbf{r} = \frac{1}{n}\,\mathbf{r}^{T}\,\mathbf{r} = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

The $\mathbf{V}$ matrix biases the risk function based on the explanatory
variables and the function $G$; basically it biases the estimate for the
residual variance.

**Question:** can we extend this to multi-respone? For a multi-response
the residuals become a matrix $\mathbf{R}$, and the risk becomes a matrix
$\mathcal{R}$ too:

$$
\begin{aligned}
\mathcal{R} & = \frac{1}{n}\,\mathbf{R}^T\,\mathbf{V}\,\mathbf{R} \\
\mathbf{R} & = \mathbf{Y} - \hat{\mathbf{Y}} \\
\hat{\mathbf{Y}} & = h(\mathbf{X})
\end{aligned}
$$

For $\mathbf{V} = \mathbf{I}$, the risk $\mathcal{R}$ becomes the variance-covariance matrix for the residuals. The $\mathbf{V}$ matrix biases
the variance-covariance matrix based on the explanatory variables and the
function $G$.

*How using it for optimization?*

In *Elements of statistical learning* Efron and Tibsharini are of the opinion that multiple responses can always be broken down into uni-variate responses and should be processed individually. However, the multi-task learning paradigm propagates the simultaneous optimization of all responses. But here, too, only the diagonal elements are usually considered; it is implicitly assumed that the covariances are all zero.

## Beispiel kernel function *G* for uni-variate explanatory variable

$$
G(z) = \exp\left[-\frac{z^2}{2\,\sigma^2}\right] 
$$

```{r}
G <- function(z, s2 = 0.1) exp(-z^2/s2)
```

```{r}
curve(G, from = -1, to = 1, xlab = "z")
```

```{r}
x <- 1:10
```

When $x_i = x_j$

```{r}
sum(G(x - x[1], 1e+1) * G(x - x[1], 1e+1))
sum(G(x - x[1], 1e+0) * G(x - x[1], 1e+0))
sum(G(x - x[1], 1e-1) * G(x - x[1], 1e-1))
```

When $x_i \neq x_j$

```{r}
sum(G(x - x[1], 1e+1) * G(x - x[2], 1e+1))
sum(G(x - x[1], 1e+0) * G(x - x[2], 1e+0))
sum(G(x - x[1], 1e-1) * G(x - x[2], 1e-1))
```

```{r}
get_V <- function(x, G, ...)
{
  # prepare output object
  n <- length(x)
  V <- matrix(ncol = n, nrow = n)
  # compute V matrix
  for (i in seq_len(n)) {
    Gi <- G(x - x[i], ...)
    for (j in seq_len(n)) {
      Gj <- G(x - x[j], ...)
      V[i,j] <- sum(Gi*Gj)
    }
  }
  # return V matrix
  V
}
```

```{r}
round(get_V(1:10, G, 1), 2)
```

```{r}
round(get_V(1:10, G, 0.1), 2)
```

## Beispiel 1: Regression

```{r}
data_generator <- function(n)
{
  x <- runif(n, min = 0, max = 10)
  y <- 2*sin(x) + 1*x + rnorm(n, mean = 0, sd = 0.5)
  data.frame(x = x,
             y = y)
}
```

```{r}
set.seed(42)
df_train <- data_generator(60)
plot(df_train)
```

```{r}
df <- data_generator(10)
round(get_V(df$x, G, 5e-4), 3)
round(get_V(df$x, G, 1e-7), 3)
```

Risk function

```{r}
emp_risk <- function(y, pred_y, V)
{
  r <- y - pred_y
  n <- length(r)
  drop(r %*% V %*% r) / n
}
```

Hypothesis function

```{r}
h <- function(x, w)
{
  #w[1] + w[2]*x + w[3]*x^2 + w[4]*x^3 + w[5]*x^4 + w[6]*x^5
  sum <- 0
  for (i in seq_along(w))
    sum <- sum + w[i]*x^(i - 1)
  sum
}
```

Driver function

```{r}
func <- function(par, df, V)
{
  pred_y <- h(df$x, par)
  emp_risk(df$y, pred_y, V)
}
```

Function for Optim mit V-Matrix.

```{r}
optim_func <- function(init_par, df_train, driver_func, G, ...)
{
  optim(
    par     = init_par,
    fn      = driver_func, 
    gr      = NULL,
    df_train, get_V(df_train$x, G, ...),
    method  = "BFGS",
    control = list(maxit = 1e4)
  )
}
```

```{r}
MSE <- function(y, pred_y)
{
  mean((y - pred_y)^2)
}
```

```{r}
df_test <- data_generator(1e6)
```

```{r}
s2_vec <- c(10^seq(-9,-5), 1:9*1e-4, 10^seq(-3,-1), 1:500, 10^seq(3,6) )
results <- vector(mode = "list", length(s2_vec))
MSE_test <- rep(NA, length(s2_vec))
start_pars <- runif(6, min = -1, max = 1)
for (i in seq_along(s2_vec)) {
  # do the fit/training
  fit <- optim_func(start_pars, df_train, func, G, s2_vec[i])
  if (fit$convergence != 0) 
    cat("Convergence problems in", i, ". round :", fit$convergence, "\n")
  # compute test error and print results
  MSE_test[i] <- MSE(df_test$y, h(df_test$x, fit$par))
  # warm start for next fit
  start_pars <- fit$par
  # store current fit
  results[[i]] <- fit
}
```

```{r, echo = FALSE}
plot(s2_vec, MSE_test, type = "b", xlab = "s2", ylab = "MSE test", 
     log = "x")
cat("\nBest s2 =", s2_vec[which.min(MSE_test)],
    "with MSE test =", min(MSE_test), "\n")
```

```{r, echo = FALSE}
plot(df_train, type = "p")
x_seq <- seq(from       = min(df_train$x),
             to         = max(df_train$x),
             length.out = 101)
cols <- colorspace::rainbow_hcl(length(s2_vec))
for (i in seq_along(s2_vec))
  lines(x   = x_seq,
        y   = h(x_seq, results[[i]]$par),
        lwd = 2,
        col = cols[i])
#legend("bottomright",
#       legend = paste("s2 = ", s2_vec),
#       col    = cols,
#       lty    = 1, 
#       lwd    = 2,
#       bty    = "n")
```

Found solution is not stable with L-BFGS-B, but is better with BFGS.

```{r}
s2_check <- rep(s2_vec[which.min(MSE_test)], 100)
results <- lapply(
  X   = s2_check,
  FUN = function(s2) optim_func(runif(6, -1, 1), df_train, func, G, s2)
)
```

```{r, echo = FALSE}
plot(df_train, type = "p")
x_seq <- seq(from = min(df_train$x),
             to = max(df_train$x),
             by = 0.1)
cols <- colorspace::rainbow_hcl(length(s2_check))
for (i in seq_along(s2_check))
  lines(x   = x_seq,
        y   = h(x_seq, results[[i]]$par),
        lwd = 2,
        col = cols[i])
```

## Example 2: binary classification

```{r}
data_generation_2 <- function(n)
{
  mean <- list(c(0,0), c(2,2))
  varcov <- list(matrix(c(1,0,0,1), ncol = 2), 
                 matrix(c(1,0,0,1), ncol = 2))
  x <- NULL
  y <- NULL
  nn <- n/2
  for (i in seq_len(2)) {
    x <- rbind(x, mnormt::rmnorm(n      = nn, 
                                 mean   = mean[[i]], 
                                 varcov = varcov[[i]]))
    y <- c(y, rep(i - 1, nn))
  }
  df <- data.frame(x, y)
  colnames(df) <- c(paste0("x", 1:2), "y")
  df
}
```

```{r}
df_train_class <- data_generation_2(100)
head(df_train_class)
```

```{r,echo = FALSE}
with(
  data = df_train_class,
  expr = {
    plot(x    = NULL,
         y    = NULL,
         xlim = range(x1),
         ylim = range(x2),
         xlab = "x1",
         ylab = "x2")
    classes <- unique(y)
    cols <- setNames(object = c("red", "blue"),
                     nm     = as.character(classes))
    for (class in classes) {
      points(x   = x1[y == class],
             y   = x2[y == class],
             col = cols[as.character(class)],
             pch = 19)
    }
  }
)
```

### G funktion multi-variate explanatory variable

```{r}
G <- function(z, s2 = 0.1)
{
  exp(-sum(z^2)/s2)
}
```

```{r}
get_V <- function(x, G, ...)
{
  n <- nrow(x)
  # prepare G tmp
  Gmat <- matrix(ncol = n, nrow = n)
  for (i in seq_len(n)) {
    for (j in seq_len(n)) {
      Gmat[i,j] <- G(x[j,] - x[i,], ...)
    }
  }
  # prepare output object  
  V <- matrix(ncol = n, nrow = n)
  # compute V matrix
  # V is symmetric: only compute upper triangular 
  # and diagonal elements
  for (i in seq(1, n)) {
    for (j in seq(i, n)) {
      V[i,j] <- sum(Gmat[i,]*Gmat[j,])
    }
  }
  # V is symmetric
  # add the missing symmetric entries of V
  for (i in seq(1, n - 1)) {
    for (j in seq(i + 1, n) ) {
      V[j,i] <- V[i,j]
    }
  }
  # return V matrix
  V
}
```

```{r}
df_test <- data_generation_2(10)
x_test <- subset(df_test, select = c("x1", "x2"))
round(get_V(x_test, G, s2 = 0.1), 3)
```

### fitting stuff

```{r}
emp_risk <- function(y, pred_y, V)
{
  r <- y - pred_y
  n <- length(r)
  drop(r %*% V %*% r) / n
}
```

Hypothesis function

```{r}
h <- function(x, w)
{
  w[1] + w[2]*x[,1] + w[3]*x[,1]^2 + w[4]*x[,2] + w[5]*x[,2]^2
  #w[1] + w[2]*x[,1] + w[3]*x[,2]
}
```

Driver function

```{r}
get_emp_risk <- function(par, x, y, V)
{
  pred_y <- h(x, par)
  risk <- emp_risk(y, pred_y, V)
  risk
}
```

Function for Optim ohne V-Matrix.

```{r}
optim_func <- function(init_par, df_train, G, s2)
{
  x_train <- subset(df_train, select = c("x1", "x2"))
  optim(
    par     = init_par,
    fn      = get_emp_risk, 
    gr      = NULL,
    x_train, df_train$y, get_V(x_train, G, s2),
    method  = "BFGS",
    control = list(maxit = 1e4, trace = 0)
  )
}
```

```{r}
get_misclass <- function(y, pred_y)
{
  mean(y != pred_y)
}
```

```{r}
df_test <- data_generation_2(1e6)
start_pars <- setNames(object = runif(5, min = -1, max = 1), 
                       nm     = paste0("w", 0:4))
s2_vec <- c(10^seq(-8,-1), 1:50, 10^seq(2,4))
misclass_vec <- rep(NA, length(s2_vec))
for (i in seq_along(s2_vec)) {
  # find best fit/hypothesis function
  fit <- optim_func(start_pars, df_train_class, G, s2 = s2_vec[i])
  # get misclassification rate
  fx <- h(subset(df_test, select = c("x1", "x2")), fit$par)
  pred_y <- ifelse(fx > 0.5, 1, 0)
  misclass_vec[i] <- get_misclass(df_test$y, pred_y)
  # warm start for next iteration
  start_pars <- fit$par
}
```

```{r, echo = FALSE}
plot(s2_vec, 100*misclass_vec, 
     type = "b", log = "x",
     ylab = "misclassification [%]",
     xlab = "s2")
cat("min misclass =", min(misclass_vec), 
    "for s2 =", s2_vec[which.min(misclass_vec)], "\n")
```

```{r,echo = FALSE}
start_pars <- setNames(object = runif(5, min = -1, max = 1), 
                       nm     = paste0("w", 0:4))
fit <- optim_func(start_pars, df_train_class, G, 
                  s2 = s2_vec[which.min(misclass_vec)])
with(
  data = df_train_class,
  expr = {
    # empty plot canvas
    plot(x    = NULL,
         y    = NULL,
         xlim = range(x1),
         ylim = range(x2),
         xlab = "x1",
         ylab = "x2")
    # plot grid
    x1_seq <- seq(from       = min(x1) - 1.1*abs(min(x1)), 
                  to         = max(x1) + 1.1*abs(max(x1)), 
                  length.out = 300)
    x2_seq <- seq(from       = min(x2) - 1.1*abs(min(x2)), 
                  to         = max(x2) + 1.1*abs(max(x2)), 
                  length.out = 300)
    grid <- expand.grid(x1 = x1_seq, x2 = x2_seq)
    y_grid <- ifelse(h(grid, fit$par) > 0.5, 1, 0)
    .filled.contour(
      x      = x1_seq,
      y      = x2_seq,
      z      = matrix(data = y_grid,
                      nrow = length(x1_seq),
                      ncol = length(x2_seq)),
      levels = c(-1,0.5,2),
      col    = c(rgb(1,0,0,0.25), rgb(0,0,1,0.25))
    )
    # plot data
    classes <- unique(y)
    #cols <- setNames(object = colorspace::rainbow_hcl(length(classes)),
    #                 nm     = as.character(classes))
    cols <- setNames(object = c("red", "blue"),
                     nm     = as.character(classes))
    for (class in classes) {
      points(x   = x1[y == class],
             y   = x2[y == class],
             col = cols[as.character(class)],
             pch = 19)
    }
    #abline(a = (0.5-fit$par[1])/fit$par[3], 
    #       b = -fit$par[2]/fit$par[3])
  }
)
```